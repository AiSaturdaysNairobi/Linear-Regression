{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is linear regression?**\n",
    "\n",
    "is an approach to modeling the relationships between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables) using a set of coefficients. In many of these models, the coefficients are referred to as \"weights\". This means when plotted on a graph, traces a straight line\n",
    "\n",
    "**dependent** : variations being studied - e.g prices\n",
    "\n",
    "**independent** : controlled inputs - e.g time, crime rate\n",
    "\n",
    "**What is meant by linear?**\n",
    "Given any change in an independent variable there will always be a corresponding change in the dependent variable.\n",
    "\n",
    "![Linear Vs Non Linear](nleverything.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a perfect world we could draw a straight line that connects all points in our data to come up with a linear function that perfectly describes a relationship between the dependent and independent variables. However due to slight variations in data, this rarely happens but instead of looking for that perfecty fitting line, we can find the line of best fit that has the least amount of error from the actual data points. This procedure of finding the line of best fit is called linear regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does linear regression work\n",
    "\n",
    "The power of linear regression comes from coming up with this line of best fit. This can be achieved through various ways but the most widely used method is called Least Squares Method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is Least Squares Method?**\n",
    "\n",
    "This is a method of approximating values of parameters to fit a function to a set of data. \n",
    "\n",
    "It works by finding the estimate that minimizes the sum of the squared distances (deviations) from the line to each observation is used to approximate a relationship that is assumed to be linear.\n",
    "\n",
    "Procedure:\n",
    "\n",
    "1. Find error term\n",
    "\\begin{equation*}\n",
    "e_i =  Y_i – (a + bX_i) \n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "\n",
    "2. We take the errors like above, square them, and then take the sum over all observations in our sample (the sample size indicated by n). \n",
    "\n",
    "Doing this, we have\n",
    "\n",
    "\\begin{equation*}\n",
    " \\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (Y_i - a - bX_i)^2 \n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "3. To get the values of a and b with the minimum error we do a partial differentiation with respect to and then b. We then end up with two equations (the two partial derivatives) with two unknowns (a and b).\n",
    "\n",
    "\n",
    "4. The last step is to solve this system of equations simultaneously for a and b. Leaving out the details, what we end up with is the following formulas for b and a:\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "b = \\frac{ \\sum_{i=1}^n (X_i - \\bar X)(Y_i -\\bar Y ) }{  \\sum_{i=1}^n (X_i - \\bar X)^2  }\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "a = \\bar X- b\\bar X\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Linear Regression\n",
    "\n",
    "Has one explanatory variable like in the one used to demonstrate the least squares method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple linear regression\n",
    "\n",
    "statistical technique that uses several explanatory variables to predict the outcome of a response variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of linear regression\n",
    "\n",
    "1. Simple\n",
    "Easy to compute and interpret\n",
    "\n",
    "2. Good interpretability\n",
    "\n",
    "Feature importance is generated at the time model building. With the help of hyperparameter lamba, you can handle features selection hence we can achieve dimensionality reduction\n",
    "\n",
    "### Disadvantages of linear regression\n",
    "\n",
    "1. Non linear data\n",
    "\n",
    "2. Outliers\n",
    "\n",
    "3. Treated equally\n",
    "\n",
    "4. Risk of overfitting is much higher for higher dimensions\n",
    "\n",
    "5. Causation vs Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions of linear regression\n",
    "https://towardsdatascience.com/verifying-the-assumptions-of-linear-regression-in-python-and-r-f4cd2907d4c0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating linear regression\n",
    "\n",
    "**R-Squared** \n",
    "\n",
    "is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model.\n",
    "\n",
    "Limitations of r-squared\n",
    "1. Does not show bias\n",
    "\n",
    "2. Does not indicate statistical significance of variables\n",
    "\n",
    "3. Works as intended for simple linear regression but increases automatically as you add new independent variables to a regression equation (even if they don’t contribute any new explanatory power to the equation)\n",
    "\n",
    "\n",
    "**Adjusted R-Squared** :\n",
    "\n",
    "The adjusted R-squared compares the descriptive power of regression models that include diverse numbers of predictors. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**P-Value** :\n",
    "\n",
    " p-value for each term tests the null hypothesis that the coefficient is equal to zero (no effect). A low p-value (< 0.05) indicates that you can reject the null hypothesis. In other words, a predictor that has a low p-value is likely to be a meaningful addition to your model because changes in the predictor's value are related to changes in the response variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
